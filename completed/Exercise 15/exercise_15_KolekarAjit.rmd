---
title: "Exercise 15: Introduction to Machine Learning"
author: "Kolekar, Ajit"
date: November 1st, 2020
output:
  
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment description
**Regression algorithms are used to predict numeric quantity while classification algorithms predict categorical outcomes. A spam filter is an example use case for a classification algorithm. The input dataset is emails labeled as either spam (i.e. junk emails) or ham (i.e. good emails). The classification algorithm uses features extracted from the emails to learn which emails fall into which category.**

**In this problem, you will use the nearest neighbors algorithm to fit a model on two simplified datasets. The first dataset (found in binary-classifier-data.csv) contains three variables; label, x, and y. The label variable is either 0 or 1 and is the output we want to predict using the x and y variables. The second dataset (found in trinary-classifier-data.csv) is similar to the first dataset except that the label variable can be 0, 1, or 2.**

**Note that in real-world datasets, your labels are usually not numbers, but text-based descriptions of the categories (e.g. spam or ham). In practice, you will encode categorical variables into numeric values.**

```{r}
library('ggplot2')
library('class')


binary_df <- read.csv("C:/Users/Shilp/Documents/GitHub/dsc520/data/binary-classifier-data.csv")


# Split data to use 80% of data to train the model and 20% of data to test the model
data_split_binary <- sample(1:nrow(binary_df), 0.8 * nrow(binary_df))
train_binary <- binary_df[data_split_binary,]
test_binary <- binary_df[-data_split_binary,]

nrow(binary_df)
nrow(train_binary)
nrow(test_binary)


trinary_df <- read.csv("C:/Users/Shilp/Documents/GitHub/dsc520/data/trinary-classifier-data.csv")


# Split data to use 80% of data to train the model and 20% of data to test the model
data_split_trinary <- sample(1:nrow(trinary_df), 0.8 * nrow(trinary_df))
train_trinary <- trinary_df[data_split_trinary,]
test_trinary <- trinary_df[-data_split_trinary,]

nrow(trinary_df)
nrow(train_trinary)
nrow(test_trinary)


```

## Question a
**Plot the data from each dataset using a scatter plot.**

```{r}
binary_factor_df <- binary_df
binary_factor_df$label <- as.factor(binary_factor_df$label)
ggplot(binary_factor_df, aes(x=x, y=y, color=label)) + geom_point()

trinary_factor_df <- trinary_df
trinary_factor_df$label <- as.factor(trinary_factor_df$label)
ggplot(trinary_factor_df, aes(x=x, y=y, color=label)) + geom_point()
```

## Question b

**The k nearest neighbors algorithm categorizes an input value by looking at the labels for the k nearest points and assigning a category based on the most common label. In this problem, you will determine which points are nearest by calculating the Euclidean distance between two points.**

**Fit a k nearest neighbors model for each dataset for k=3, k=5, k=10, k=15, k=20, and k=25. Compute the accuracy of the resulting models for each value of k. Plot the results in a graph where the x-axis is the different values of k and the y-axis is the accuracy of the model.**

Euclidean Distance:

```{r}
# install.packages('TSdist')
library('TSdist')
EuclideanDistance(binary_df$x,binary_df$y)
EuclideanDistance(trinary_df$x,trinary_df$y)
```

Binary dataset:

```{r}
binary_glm <- glm(label ~ x + y, data=binary_df, family = binomial)

# extract 1st column of train dataset because it will be used as 'cl' argument in knn function.
target_category <- binary_df[data_split_binary,1]

# extract 1st column if test dataset to measure the accuracy
test_category <- binary_df[-data_split_binary,1]

k_values<- c(3, 5, 10, 15, 20, 25)
Accuracy <- NULL

for (i in 1:length(k_values))
{
  test_pred <- knn(train_binary,test_binary,cl=target_category,k=k_values[i])
  confmatrix <- table(test_category,test_pred)
  accuracy <- (confmatrix[[1,1]] + confmatrix[[2,2]]) / sum(confmatrix)
  Accuracy <- c(Accuracy, round((accuracy * 100), digits=2))
}  

binary_results_df <- data.frame(k_values, Accuracy)
binary_results_df

ggplot(binary_results_df, aes(x=k_values, y=Accuracy)) + geom_point() + geom_line(colour="red")

```  

Trinary dataset:

```{r}
trinary_glm <- glm(label ~ x + y, data=trinary_df, family = poisson)

# extract 1st column of train dataset because it will be used as 'cl' argument in knn function.
target_category <- trinary_df[data_split_trinary,1]

# extract 1st column if test dataset to measure the accuracy
test_category <- trinary_df[-data_split_trinary,1]

k_values<- c(3, 5, 10, 15, 20, 25)
Accuracy <- NULL

for (i in 1:length(k_values))
{
  test_pred <- knn(train_trinary,test_trinary,cl=target_category,k=k_values[i])
  confmatrix <- table(test_category,test_pred)
  accuracy <- (confmatrix[[1,1]] + confmatrix[[2,2]]) / sum(confmatrix)
  Accuracy <- c(Accuracy, round((accuracy * 100), digits=2))
}  

trinary_results_df <- data.frame(k_values, Accuracy)
trinary_results_df

ggplot(trinary_results_df, aes(x=k_values, y=Accuracy)) + geom_point() + geom_line(colour="red")

```  


## Question c
**Looking back at the plots of the data, do you think a linear classifier would work well on these datasets?**

By looking at the plots of the data, linear classifier would not work well with both binary data set and trinary data set, since the data points are scattered all over. It is not possible to draw a straight line through the data points that would correctly represent the data. The data points are in clusters, so K-Nearest Neighbors classifier would work well for these data sets. 




